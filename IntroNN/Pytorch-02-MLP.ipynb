{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST + MLP\n",
    "\n",
    "In this notebook, we design and train a Multi-Layer Perceptrons (MLP) for hand-written digit classification task. We use MNIST dataset that contains 28x28 pixel images of a hand-written digit (0 to 9, so 10 classification targets). \n",
    "\n",
    "## Goals\n",
    "1. Get familiar with MNIST dataset\n",
    "2. Build a pipeline to stream data for SGD\n",
    "3. Design MLP and train on MNIST\n",
    "\n",
    "Let's start with usual import!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "mpl.rcParams['font.size'] = 16\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "SEED=12345\n",
    "_=np.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNIST Dataset\n",
    "MNIST is widely used for an introductory machine learning (ML) courses/lectures. Most, if not all, ML libraries provide an easy way (API) to access MNIST and many publicly available dataset. This is true in `pytorch` as well. MNIST dataset in `Dataset` instance is available from `torchvision`. \n",
    "\n",
    "A `torchvision` is a supporting module that has many image-related APIs including an interface (and management) of MNIST dataset. Let's see how we can construct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "# Data file download directory\n",
    "LOCAL_DATA_DIR = './mnist-data'\n",
    "# Use prepared data handler from pytorch (torchvision)\n",
    "dataset = datasets.MNIST(LOCAL_DATA_DIR, train=True, download=True,\n",
    "                         transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, MNIST is also a type `Dataset` (how? through class inheritance). All torch `Dataset` instance have tow useful and common functions: the length representations and data element access via index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(dataset)  )\n",
    "print( type(dataset[0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That being said, how each data element is presented depends on a particular `Dataset` implementation. In case of MNIST, it is a tuple of length 2: **data** and **label**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTRY=0\n",
    "data, label = dataset[ENTRY]\n",
    "print('Type of data  :', type(data),  'shape', data.shape)\n",
    "print('Type of label :', type(label), 'value', label)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Draw data\n",
    "data = data.view(data.shape[1:])\n",
    "plt.imshow(data,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"dataloader\"></a>\n",
    "## 2. Streaming MNIST data using `DataLoader`\n",
    "\n",
    "Pytorch (and any other ML libraries out there) provides a generalized tool to interface an iteratable data instance called `DataLoader`. This is a useful tool for streaming data in a typical ML workflow. \n",
    "\n",
    "1. iteratable batching of data\n",
    "2. custom data sampling (random provided by default)\n",
    "3. custom data pre-processing\n",
    "4. prallel data reading\n",
    "\n",
    "Let's play a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data = np.arange(10)\n",
    "\n",
    "print('\\nVanilla data loader')\n",
    "loader = DataLoader(data)\n",
    "for index, batch_data in enumerate(loader):\n",
    "    print(index,batch_data)\n",
    "\n",
    "print('\\n... batch_size set to 3')\n",
    "loader = DataLoader(data, batch_size=3)\n",
    "for index, batch_data in enumerate(loader):\n",
    "    print(index,batch_data)\n",
    "    \n",
    "print('\\n... + drop_last set to True')\n",
    "loader = DataLoader(data, batch_size=3, drop_last=True)\n",
    "for index, batch_data in enumerate(loader):\n",
    "    print(index,batch_data)\n",
    "\n",
    "print('\\n... + shuffle set to True')\n",
    "loader = DataLoader(data, batch_size=3, drop_last=True, shuffle=True)\n",
    "for index, batch_data in enumerate(loader):\n",
    "    print(index,batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap the data loading loop by another loop so that we can continue iterating our dataset indefinitely for optimizing our model over 1 epoch. But if you want a reproducibility of the data subsets created by `DataLoader`, you can set the random seed explicitly as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size=3, drop_last=True, shuffle=True)\n",
    "\n",
    "for i in range(2):\n",
    "    print('Loop',i)\n",
    "    for index, batch_data in enumerate(loader):\n",
    "        print(index,batch_data)\n",
    "    \n",
    "print('\\nRepeating by setting the seed')\n",
    "for i in range(2):\n",
    "    print('Loop',i)\n",
    "    torch.manual_seed(1)\n",
    "    for index, batch_data in enumerate(loader):\n",
    "        print(index,batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating your own dataset\n",
    "\n",
    "For your research, often we want to create our own dataset with handy utilities. You can define your own dataset and use with the `DataLoader` as long as it's either iterable (i.e. implements `__iter__` built-in method) or supports `len` function and a random index access (i.e. `__len__` and `__getitem__` support).\n",
    "\n",
    "Let's create a trivial example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class toy_dataset:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._data = tuple(range(100))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        time.sleep(0.1)\n",
    "        return self._data[index]\n",
    "    \n",
    "data = toy_dataset()\n",
    "\n",
    "loader = DataLoader(data,batch_size=10,shuffle=True)\n",
    "\n",
    "for index, batch_data in enumerate(loader):\n",
    "    print(index,batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which is very slow because our dataset takes 0.1 second per data element access :( While we put `time.sleep(0.1)` intentionally, this (=slow-down for per-sample data yield) can happen in reality. Perhaps you are forced to read each data sample from a file-system (i.e. \"disk\") each time because your data is too big to be stored in memory, or maybe you have to pre-process your data with complicated procedures. \n",
    "\n",
    "There is a simple way to automatize this by setting another `DataLoader` constructor argument: `num_workers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data,batch_size=10,shuffle=True,num_workers=5)\n",
    "\n",
    "for index, batch_data in enumerate(loader):\n",
    "    print(index,batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `num_workers=5`, we told `DataLoader` to use 5 separate _workers_ responsible for fetching data in parallel. This is one of the simplest forms to parallelize your data streaming.  In this tutorial, we don't cover a multi-GPU nor multi-node-multi-GPU data distribution. But tools are provided for those as well (e.g. Pytorch provides `DistributeDataParallel`, and there are also external wrapper tools like `Horovod`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating `DataLoader` with the MNIST dataset\n",
    "\n",
    "Now let's create MNIST dataset with `batch_size=32`, `shuffle=True`, `num_workers=4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from kmi.iotools.collates import MNISTCollate\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=32,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=4,\n",
    "                                     pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above introduced only 1 additional argument: `pin_memory=True` can speed up data transfer to GPU by avoiding a necessiry to copy data from pageable memory to page-locked (pinned) memory. Read [here](https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/) for more details. If you are not sure about the details, set to `True` when using GPU. \n",
    "\n",
    "So let's play with it! First of all, it has the concept of \"length\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length of DataLoader:',len(loader))\n",
    "print('By the way, batch size * length =', 20 * len(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the data total statistics is 60,000 which coincides with the length of `DataLoader` instance and the batch size where the latter is the unit of batch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator for playin in this notebook\n",
    "from itertools import cycle\n",
    "iter = cycle(loader)\n",
    "\n",
    "for i in range(10):\n",
    "    batch = next(iter)    \n",
    "    print('Iteration',i)\n",
    "    print(batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and this is how `data` looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of an image batch data',batch[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which is quite naturally 20 of 28x28 image\n",
    "\n",
    "## 3. MNIST classification using MLP\n",
    "\n",
    "Let's try classifying 10 hand-written digits using MLP!\n",
    "\n",
    "### 3.1 Model definition\n",
    "We follow a similar approach we have taken in the previous notebook where we tried a logistic regression using 2-layers MLP with LeakyReLU activation function between two layers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_filters=16):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        # MLP w/ 2 hidden layers, 128 neurons each\n",
    "        self._classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28*28, num_filters), \n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(num_filters,10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Make 2d image into 1D array\n",
    "        x_1d = x.view(-1, np.prod(x.size()[1:]))\n",
    "        return self._classifier(x_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Train loop function\n",
    "Next, let's define a train loop function. This is also mostly copy-and-paste from the previous notebook. A small modification include an option to specify gpu v.s. cpu mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch(data_loader, model, num_iterations=100, lr=0.001, optimizer='SGD', gpu=False):\n",
    "    # Create a Binary-Cross-Entropy (BCE) loss module\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # Create an optimizer\n",
    "    optimizer = getattr(torch.optim,optimizer)(model.parameters(),lr=lr)\n",
    "    # Now we run the training!\n",
    "    loss_v=[]\n",
    "    while num_iterations > 0:\n",
    "        for data,label in data_loader:\n",
    "            \n",
    "            if gpu:\n",
    "                data,label = data.cuda(),label.cuda()\n",
    "            # Prediction\n",
    "            prediction = model(data)\n",
    "            # Compute loss\n",
    "            loss = criterion(prediction, label)\n",
    "            # Update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Record loss\n",
    "            loss_v.append(loss.item())\n",
    "            # Brake if we consumed all iteration counts\n",
    "            num_iterations -= 1\n",
    "            if num_iterations < 1:\n",
    "                break\n",
    "        \n",
    "    return np.array(loss_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!\n",
    "\n",
    "Let's train for 4000 steps using Adam optimizer. Also, the number of filters is default = 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = MLP()\n",
    "\n",
    "loss = train_torch(loader, model_a, 4000, optimizer='Adam')\n",
    "\n",
    "plt.plot(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a pretty large fluctuation! Often it's useful to look at a moothed loss curve by taking an average value with the neighbor data points. Let's try this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute moving average\n",
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.plot(moving_average(loss,20))\n",
    "print('last 20 average',np.mean(loss[-20:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Exercise A\n",
    "\n",
    "1. Repeat the same training for `model_b` = `MLP` with 32 filters. \n",
    "2. Plot the loss curve with 20 neighbor-points average.\n",
    "3. Compare the los value of the last 20 steps against the `model_a`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Exercise B\n",
    "\n",
    "1. Repeat the exercise 1 and measure how long it takes in wall-time.\n",
    "2. Repeat again, but this time with GPU enabled, and measure how long it takes.\n",
    "\n",
    "You probably see a very similar amount of time taken for both cases. This is because parallelizable fraction of computation is not dominating in the overall computing time (and especially when most time may be taken to copy data onto gpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on test dataset\n",
    "Both models seem to be trained OK. Let's benchmark their performance using the test dataset! Pytorch provides 10,000 MNIST dataset that is separate from the training set by simply setting the flag `train=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use prepared data handler from pytorch (torchvision)\n",
    "test_dataset = datasets.MNIST(LOCAL_DATA_DIR, train=False, download=True,\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4,\n",
    "                                          pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference loop function\n",
    "\n",
    "Let's now write a function to run the inference. This would be similar to the training loop. A key difference is to use a scope `with torch.set_grad_enabled(False)` which disables gradient calculation and caching of intermediate data for it. This results in less memory usage, so you should do this when you run your model for inference and not training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model,loader,gpu=False):\n",
    "\n",
    "    from scipy.special import softmax\n",
    "\n",
    "    prediction_v = []\n",
    "    label_v      = []\n",
    "    softmax_v    = []\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        idx=0\n",
    "        for data,label in loader:\n",
    "            if gpu:\n",
    "                data,label = data.cuda(), label.cuda()\n",
    "            prediction   = model(data).cpu().numpy()\n",
    "            prediction_v.append ( np.argmax(prediction,axis=1)    )\n",
    "            label_v.append      ( label.cpu().numpy().reshape(-1) )\n",
    "            s = softmax(prediction,axis=1)\n",
    "            softmax_v.append    (s)\n",
    "            idx +=1\n",
    "    return np.concatenate(prediction_v), np.concatenate(label_v), np.concatenate(softmax_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Exercise C\n",
    "1. Run the inference for `model_a` and `model_b`.\n",
    "2. Using the results, compute the accuracy over the whole test dataset for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Exercise D\n",
    "\n",
    "1. Count the number of parameters in our `model_a`\n",
    "2. How about `model_b`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
